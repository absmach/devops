# Copyright (c) Mainflux
# SPDX-License-Identifier: Apache-2.0

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: {{ .Release.Name }}-view-clusterrole
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- kind: ServiceAccount
  name: default
  namespace: {{ .Release.Namespace }}
---
apiVersion: v1
kind: Service
metadata:
  name: {{ .Release.Name }}-adapter-mqtt
  labels:
    app: {{ .Release.Name }}
    component: adapter-mqtt
spec:
  selector:
    app: {{ .Release.Name }}
    component: adapter-mqtt
  ports:
    - port: 1883
      protocol: TCP
      name: "1883"
    - port: 8880
      protocol: TCP
      name: "8880"
    - port: 8080
      protocol: TCP
      name: "8080"
  clusterIP: None
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-adapter-mqtt
data:
  start_vernemq: |
    #!/usr/bin/env bash
    echo "Starting mqtt-verne ..."
    sleep 5

    IP_ADDRESS=$(ip -4 addr show ${DOCKER_NET_INTERFACE:-eth0} | grep -oE '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}' | sed -e "s/^[[:space:]]*//" | head -n 1)
    IP_ADDRESS=${DOCKER_IP_ADDRESS:-${IP_ADDRESS}}

    # Ensure the Erlang node name is set correctly
    if env | grep "DOCKER_VERNEMQ_NODENAME" -q; then
        sed -i.bak -r "s/-name VerneMQ@.+/-name VerneMQ@${DOCKER_VERNEMQ_NODENAME}/" /vernemq/etc/vm.args
    else
        if [ -n "$DOCKER_VERNEMQ_SWARM" ]; then
            NODENAME=$(hostname -i)
            sed -i.bak -r "s/VerneMQ@.+/VerneMQ@${NODENAME}/" /etc/vernemq/vm.args
        else
            sed -i.bak -r "s/-name VerneMQ@.+/-name VerneMQ@${IP_ADDRESS}/" /vernemq/etc/vm.args
        fi
    fi

    if env | grep "DOCKER_VERNEMQ_DISCOVERY_NODE" -q; then
        discovery_node=$DOCKER_VERNEMQ_DISCOVERY_NODE
        if [ -n "$DOCKER_VERNEMQ_SWARM" ]; then
            tmp=''
            while [[ -z "$tmp" ]]; do
                tmp=$(getent hosts tasks.$discovery_node | awk '{print $1}' | head -n 1)
                sleep 1
            done
            discovery_node=$tmp
        fi
        if [ -n "$DOCKER_VERNEMQ_COMPOSE" ]; then
            tmp=''
            while [[ -z "$tmp" ]]; do
                tmp=$(getent hosts $discovery_node | awk '{print $1}' | head -n 1)
                sleep 1
            done
            discovery_node=$tmp
        fi

        sed -i.bak -r "/-eval.+/d" /vernemq/etc/vm.args 
        echo "-eval \"vmq_server_cmd:node_join('VerneMQ@$discovery_node')\"" >> /vernemq/etc/vm.args
    fi

    # If you encounter "SSL certification error (subject name does not match the host name)", you may try to set DOCKER_VERNEMQ_KUBERNETES_INSECURE to "1".
    insecure=""
    if env | grep "DOCKER_VERNEMQ_KUBERNETES_INSECURE" -q; then
        insecure="--insecure"
    fi

    if env | grep "DOCKER_VERNEMQ_DISCOVERY_KUBERNETES" -q; then
        DOCKER_VERNEMQ_KUBERNETES_CLUSTER_NAME=${DOCKER_VERNEMQ_KUBERNETES_CLUSTER_NAME:-cluster.local}
        # Let's get the namespace if it isn't set
        DOCKER_VERNEMQ_KUBERNETES_NAMESPACE=${DOCKER_VERNEMQ_KUBERNETES_NAMESPACE:-`cat /var/run/secrets/kubernetes.io/serviceaccount/namespace`}
        # Let's set our nodename correctly
        VERNEMQ_KUBERNETES_SUBDOMAIN=${DOCKER_VERNEMQ_KUBERNETES_SUBDOMAIN:-$(curl -X GET $insecure --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetes.default.svc.$DOCKER_VERNEMQ_KUBERNETES_CLUSTER_NAME/api/v1/namespaces/$DOCKER_VERNEMQ_KUBERNETES_NAMESPACE/pods?labelSelector=$DOCKER_VERNEMQ_KUBERNETES_LABEL_SELECTOR -H "Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" | jq '.items[0].spec.subdomain' | sed 's/"//g' | tr '\n' '\0')}
        if [ $VERNEMQ_KUBERNETES_SUBDOMAIN == "null" ]; then
            VERNEMQ_KUBERNETES_HOSTNAME=${MY_POD_NAME}.${DOCKER_VERNEMQ_KUBERNETES_NAMESPACE}.svc.${DOCKER_VERNEMQ_KUBERNETES_CLUSTER_NAME}
        else
            VERNEMQ_KUBERNETES_HOSTNAME=${MY_POD_NAME}.${VERNEMQ_KUBERNETES_SUBDOMAIN}.${DOCKER_VERNEMQ_KUBERNETES_NAMESPACE}.svc.${DOCKER_VERNEMQ_KUBERNETES_CLUSTER_NAME}
        fi

        sed -i.bak -r "s/VerneMQ@.+/VerneMQ@${VERNEMQ_KUBERNETES_HOSTNAME}/" /vernemq/etc/vm.args
        # Hack into K8S DNS resolution (temporarily)
        kube_pod_names=$(curl -X GET $insecure --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetes.default.svc.$DOCKER_VERNEMQ_KUBERNETES_CLUSTER_NAME/api/v1/namespaces/$DOCKER_VERNEMQ_KUBERNETES_NAMESPACE/pods?labelSelector=$DOCKER_VERNEMQ_KUBERNETES_LABEL_SELECTOR -H "Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" | jq '.items[].spec.hostname' | sed 's/"//g' | tr '\n' ' ')
        echo "=== Discovered kube_pod_names: $kube_pod_names"
        for kube_pod_name in $kube_pod_names;
        do
            if [ $kube_pod_name == "null" ]
                then
                    echo "Kubernetes discovery selected, but no pods found. Maybe we're the first?"
                    echo "Anyway, we won't attempt to join any cluster."
                    break
            fi
            if [ $kube_pod_name != $MY_POD_NAME ]
                then
                    echo "Will join an existing Kubernetes cluster with discovery node at ${kube_pod_name}.${VERNEMQ_KUBERNETES_SUBDOMAIN}.${DOCKER_VERNEMQ_KUBERNETES_NAMESPACE}.svc.${DOCKER_VERNEMQ_KUBERNETES_CLUSTER_NAME}"
                    echo "-eval \"vmq_server_cmd:node_join('VerneMQ@${kube_pod_name}.${VERNEMQ_KUBERNETES_SUBDOMAIN}.${DOCKER_VERNEMQ_KUBERNETES_NAMESPACE}.svc.${DOCKER_VERNEMQ_KUBERNETES_CLUSTER_NAME}')\"" >> /vernemq/etc/vm.args
                    break
            fi
        done
    fi

    if [ -f /vernemq/etc/vernemq.conf.local ]; then
        cp /vernemq/etc/vernemq.conf.local /vernemq/etc/vernemq.conf
        sed -i -r "s/###IPADDRESS###/${IP_ADDRESS}/" /vernemq/etc/vernemq.conf
    else
        sed -i '/########## Start ##########/,/########## End ##########/d' /vernemq/etc/vernemq.conf

        echo "########## Start ##########" >> /vernemq/etc/vernemq.conf

        env | grep DOCKER_VERNEMQ | grep -v 'DISCOVERY_NODE\|KUBERNETES\|SWARM\|COMPOSE\|DOCKER_VERNEMQ_USER' | cut -c 16- | awk '{match($0,/^[A-Z0-9_]*/)}{print tolower(substr($0,RSTART,RLENGTH)) substr($0,RLENGTH+1)}' | sed 's/__/./g' >> /vernemq/etc/vernemq.conf

        users_are_set=$(env | grep DOCKER_VERNEMQ_USER)
        if [ ! -z "$users_are_set" ]; then
            echo "vmq_passwd.password_file = /vernemq/etc/vmq.passwd" >> /vernemq/etc/vernemq.conf
            touch /vernemq/etc/vmq.passwd
        fi

        for vernemq_user in $(env | grep DOCKER_VERNEMQ_USER); do
            username=$(echo $vernemq_user | awk -F '=' '{ print $1 }' | sed 's/DOCKER_VERNEMQ_USER_//g' | tr '[:upper:]' '[:lower:]')
            password=$(echo $vernemq_user | awk -F '=' '{ print $2 }')
            /vernemq/bin/vmq-passwd /vernemq/etc/vmq.passwd $username <<EOF
    $password
    $password
    EOF
        done

        echo "erlang.distribution.port_range.minimum = 9100" >> /vernemq/etc/vernemq.conf
        echo "erlang.distribution.port_range.maximum = 9109" >> /vernemq/etc/vernemq.conf
        echo "listener.tcp.default = ${IP_ADDRESS}:1883" >> /vernemq/etc/vernemq.conf
        echo "listener.ws.default = ${IP_ADDRESS}:8080" >> /vernemq/etc/vernemq.conf
        echo "listener.vmq.clustering = ${IP_ADDRESS}:44053" >> /vernemq/etc/vernemq.conf
        echo "listener.http.metrics = ${IP_ADDRESS}:8888" >> /vernemq/etc/vernemq.conf

        echo "########## End ##########" >> /vernemq/etc/vernemq.conf
    fi

    echo "Checking configuration file..."
    # Check configuration file

    /vernemq/bin/vernemq config generate | tee /tmp/config.out | grep error

    if [ $? -ne 1 ]; then
        echo "configuration error, exit"
        echo "$(cat /tmp/config.out)"
        exit $?
    fi

    pid=0

    # SIGUSR1-handler
    siguser1_handler() {
        echo "stopped"
    }

    # SIGTERM-handler
    sigterm_handler() {
        if [ $pid -ne 0 ]; then
            # this will stop the VerneMQ process
            /vernemq/bin/vmq-admin cluster leave node=VerneMQ@$IP_ADDRESS -k > /dev/null
            wait "$pid"
        fi
        exit 143; # 128 + 15 -- SIGTERM
    }

    # Setup OS signal handlers
    trap 'siguser1_handler' SIGUSR1
    trap 'sigterm_handler' SIGTERM

    # Start VerneMQ
    /vernemq/bin/vernemq console -noshell -noinput $@
    pid=$(ps aux | grep '[b]eam.smp' | awk '{print $2}')
    wait $pid

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ .Release.Name }}-adapter-mqtt
spec:
  serviceName: {{ .Release.Name }}-adapter-mqtt
  selector:
    matchLabels:
      app: {{ .Release.Name }}
      component: adapter-mqtt
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}
        component: adapter-mqtt
    spec:
      volumes:
        - name: start-vernemq
          configMap:
            name: {{ .Release.Name }}-adapter-mqtt
            defaultMode: 0755
      containers:
        - volumeMounts:
            - name: start-vernemq
              mountPath: /usr/sbin/start_vernemq
              subPath: start_vernemq
          env:
            - name: MF_MQTT_ADAPTER_LOG_LEVEL
              value: debug
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MF_MQTT_INSTANCE_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MF_MQTT_ADAPTER_ES_URL
              value: tcp://{{ .Release.Name }}-redis-mqtt-master:6379
            - name: MF_NATS_URL
              value: nats://{{ .Release.Name }}-nats-client:4222
            - name: MF_THINGS_AUTH_GRPC_URL
              value: http://{{ .Release.Name }}-envoy:8183
            - name: DOCKER_VERNEMQ_PLUGINS__VMQ_PASSWD
              value: "off"
            - name: DOCKER_VERNEMQ_PLUGINS__VMQ_ACL
              value: "off"
            - name: DOCKER_VERNEMQ_PLUGINS__MFX_AUTH
              value: "on"
            - name: DOCKER_VERNEMQ_PLUGINS__MFX_AUTH__PATH
              value: /mainflux/_build/default
            - name: DOCKER_VERNEMQ_LOG__CONSOLE__LEVEL
              value: {{ default .Values.defaults.logLevel .Values.adapter_mqtt.logLevel }}
            - name: MF_MQTT_VERNEMQ_GRPC_POOL_SIZE
              value: "1000"
            - name: DOCKER_VERNEMQ_ALLOW_ANONYMOUS
              value: "on"
            - name: DOCKER_VERNEMQ_DISCOVERY_KUBERNETES
              value: "1"
            - name: DOCKER_VERNEMQ_KUBERNETES_LABEL_SELECTOR
              value: "app={{ .Release.Name }},component=adapter-mqtt"
          # image: mainflux/mqtt-verne:0.10.0
          image: "{{ default .Values.defaults.image.repository .Values.adapter_mqtt.image.repository }}/mqtt-verne:{{ default .Values.defaults.image.tag .Values.adapter_mqtt.image.tag }}"
          imagePullPolicy: {{ default .Values.defaults.image.pullPolicy .Values.adapter_mqtt.image.pullPolicy }}
          name: {{ .Release.Name }}-adapter-mqtt
          ports:
            - containerPort: 1883
              protocol: TCP
            - containerPort: 8080
              protocol: TCP
            - containerPort: 8880
              protocol: TCP
            - containerPort: 8888
              protocol: TCP
            - containerPort: 44053
              protocol: TCP
            - containerPort: 4369
              protocol: TCP
            - containerPort: 9100
              protocol: TCP
            - containerPort: 9101
              protocol: TCP
            - containerPort: 9102
              protocol: TCP
            - containerPort: 9103
              protocol: TCP
            - containerPort: 9104
              protocol: TCP
            - containerPort: 9105
              protocol: TCP
            - containerPort: 9106
              protocol: TCP
            - containerPort: 9107
              protocol: TCP
            - containerPort: 9108
              protocol: TCP
            - containerPort: 9109
              protocol: TCP
          stdin: true
          tty: true
          readinessProbe:
            tcpSocket:
              port: 44053
            initialDelaySeconds: 10
            periodSeconds: 5
      dnsPolicy: ClusterFirst
      restartPolicy: Always
